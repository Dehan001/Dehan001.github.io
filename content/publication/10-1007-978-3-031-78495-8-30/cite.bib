@inproceedings{10.1007/978-3-031-78495-8_30,
 abstract = {Delving into the realm of Bangla text analysis, our study ventures to unlock the potential of both Large and Tiny Language Models across a range of classification tasks, from deciphering sentiment to detecting sarcasm, emotion, hate speech, and fake news. In a linguistic landscape where resources are scarce, we fill a crucial gap by meticulously evaluating model performance. Our findings unveil Gemma-2B and Bangla-BERT as top performers, with Gemma-2B excelling in detecting hate speech and sarcasm, while BanglaBERT shines in sentiment analysis and emotion detection. Notably, TinyLlama emerges as a standout, showcasing exceptional prowess in fake news detection. We emphasize the importance of selecting models attuned to the intricacies of Bangla text, with Gemma-2B, TinyLlama, and BanglaBERT exhibiting notable accuracy improvements, surpassing other contenders. Furthermore, we uncover performance disparities influenced by dataset origins, with Bangla Language Models adept at capturing social media sentiments, and Large Language Models excelling in identifying misinformation and abusive language in formal sources. Our comparison with ChatGPT's zero-shot prompting underscores the necessity for advanced NLP methodologies. By spotlighting TinyLLM, we showcase the potential of advanced NLP in Bangla text classification, paving the way for broader advancements in NLP research.},
 address = {Cham},
 author = {Dehan, Farhan Noor
and Fahim, Md
and Rahman, A. K. M. Mahabubur
and Amin, M. Ashraful
and Ali, Amin Ahsan},
 booktitle = {Pattern Recognition},
 editor = {Antonacopoulos, Apostolos
and Chaudhuri, Subhasis
and Chellappa, Rama
and Liu, Cheng-Lin
and Bhattacharya, Saumik
and Pal, Umapada},
 isbn = {978-3-031-78495-8},
 pages = {472--487},
 publisher = {Springer Nature Switzerland},
 title = {TinyLLM Efficacy in Low-Resource Language: An Experiment on Bangla Text Classification Task},
 year = {2025}
}
