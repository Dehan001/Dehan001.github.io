---
title: 'TinyLLM Efficacy in Low-Resource Language: An Experiment on Bangla Text Classification
  Task'
authors:
- Farhan Noor Dehan
- Md Fahim
- A. K. M. Mahabubur Rahman
- M. Ashraful Amin
- Amin Ahsan Ali
date: '2025-01-01'
publishDate: '2024-12-09T19:33:07.698131Z'
publication_types:
- paper-conference
publication: '*Pattern Recognition*'
abstract: Delving into the realm of Bangla text analysis, our study ventures to unlock
  the potential of both Large and Tiny Language Models across a range of classification
  tasks, from deciphering sentiment to detecting sarcasm, emotion, hate speech, and
  fake news. In a linguistic landscape where resources are scarce, we fill a crucial
  gap by meticulously evaluating model performance. Our findings unveil Gemma-2B and
  Bangla-BERT as top performers, with Gemma-2B excelling in detecting hate speech
  and sarcasm, while BanglaBERT shines in sentiment analysis and emotion detection.
  Notably, TinyLlama emerges as a standout, showcasing exceptional prowess in fake
  news detection. We emphasize the importance of selecting models attuned to the intricacies
  of Bangla text, with Gemma-2B, TinyLlama, and BanglaBERT exhibiting notable accuracy
  improvements, surpassing other contenders. Furthermore, we uncover performance disparities
  influenced by dataset origins, with Bangla Language Models adept at capturing social
  media sentiments, and Large Language Models excelling in identifying misinformation and
  abusive language in formal sources. Our comparison with ChatGPT's zero-shot prompting
  underscores the necessity for advanced NLP methodologies. By spotlighting TinyLLM,
  we showcase the potential of advanced NLP in Bangla text classification, paving
  the way for broader advancements in NLP research.
---
